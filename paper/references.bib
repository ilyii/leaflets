%MARKER:OCR
%E2E Pipelines
@inproceedings{tesseract,
  author    = {Smith, R.},
  booktitle = {Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)},
  title     = {An Overview of the Tesseract OCR Engine},
  year      = {2007},
  volume    = {2},
  number    = {},
  pages     = {629-633},
  keywords  = {Optical character recognition software;Search engines;Testing;Open source software;Text recognition;Filters;Prototypes;Independent component analysis;Pipelines;Inspection},
  doi       = {10.1109/ICDAR.2007.4376991}
}

@misc{ppocrv2,
  title         = {PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System},
  author        = {Yuning Du and Chenxia Li and Ruoyu Guo and Cheng Cui and Weiwei Liu and Jun Zhou and Bin Lu and Yehua Yang and Qiwen Liu and Xiaoguang Hu and Dianhai Yu and Yanjun Ma},
  year          = {2021},
  eprint        = {2109.03144},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2109.03144}
}

%Text Detection
%DB
@article{liao2019,
  title        = {Real-time Scene Text Detection with Differentiable Binarization},
  url          = {http://arxiv.org/abs/1911.08947},
  doi          = {10.48550/arXiv.1911.08947},
  abstractnote = {Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of DB on five benchmark datasets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by DB are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of ResNet-18, our detector achieves an F-measure of 82.8, running at 62 FPS, on the MSRA-TD500 dataset. Code is available at: https://github.com/MhLiao/DB},
  note         = {arXiv:1911.08947 [cs]},
  number       = {arXiv:1911.08947},
  publisher    = {arXiv},
  author       = {Liao, Minghui and Wan, Zhaoyi and Yao, Cong and Chen, Kai and Bai, Xiang},
  year         = {2019},
  month        = dec
}

%DB++
@misc{liao2022,
  title         = {Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion},
  author        = {Minghui Liao and Zhisheng Zou and Zhaoyi Wan and Cong Yao and Xiang Bai},
  year          = {2022},
  eprint        = {2202.10304},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2202.10304}
}

%Text Recognition
@misc{li2022,
  title         = {TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models},
  author        = {Minghao Li and Tengchao Lv and Jingye Chen and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},
  year          = {2022},
  eprint        = {2109.10282},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2109.10282}
}

%%Scene TR
@misc{shi2015,
  title         = {An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition},
  author        = {Baoguang Shi and Xiang Bai and Cong Yao},
  year          = {2015},
  eprint        = {1507.05717},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1507.05717}
}

@misc{gao2017,
  title         = {Reading Scene Text with Attention Convolutional Sequence Modeling},
  author        = {Yunze Gao and Yingying Chen and Jinqiao Wang and Hanqing Lu},
  year          = {2017},
  eprint        = {1709.04303},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1709.04303}
}

@misc{sheng2019,
  title         = {NRTR: A No-Recurrence Sequence-to-Sequence Model For Scene Text Recognition},
  author        = {Fenfen Sheng and Zhineng Chen and Bo Xu},
  year          = {2019},
  eprint        = {1806.00926},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1806.00926}
}

%%Rectification
@article{shi2019,
  author   = {Shi, Baoguang and Yang, Mingkun and Wang, Xinggang and Lyu, Pengyuan and Yao, Cong and Bai, Xiang},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {ASTER: An Attentional Scene Text Recognizer with Flexible Rectification},
  year     = {2019},
  volume   = {41},
  number   = {9},
  pages    = {2035-2048},
  keywords = {Text recognition;Character recognition;Detectors;Decoding;Recurrent neural networks;Proposals;Scene text recognition;thin-plate spline;image transformation;sequence-to-sequence learning},
  doi      = {10.1109/TPAMI.2018.2848939}
}

@misc{zhan2019,
  title         = {ESIR: End-to-end Scene Text Recognition via Iterative Image Rectification},
  author        = {Fangneng Zhan and Shijian Lu},
  year          = {2019},
  eprint        = {1812.05824},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1812.05824}
}

%MARKER:VISUAL DOCUMENT UNDERSTANDING (VQA, OCR)
%% E2E
@misc{appalaraju2021docformerendtoendtransformerdocument,
  title         = {DocFormer: End-to-End Transformer for Document Understanding},
  author        = {Srikar Appalaraju and Bhavan Jasani and Bhargava Urala Kota and Yusheng Xie and R. Manmatha},
  year          = {2021},
  eprint        = {2106.11539},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2106.11539}
}

@misc{peng2022ernielayoutlayoutknowledgeenhanced,
  title         = {ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding},
  author        = {Qiming Peng and Yinxu Pan and Wenjin Wang and Bin Luo and Zhenyu Zhang and Zhengjie Huang and Teng Hu and Weichong Yin and Yongfeng Chen and Yin Zhang and Shikun Feng and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},
  year          = {2022},
  eprint        = {2210.06155},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2210.06155}
}

@misc{li2021structextstructuredtextunderstanding,
  title         = {StrucTexT: Structured Text Understanding with Multi-Modal Transformers},
  author        = {Yulin Li and Yuxi Qian and Yuchen Yu and Xiameng Qin and Chengquan Zhang and Yan Liu and Kun Yao and Junyu Han and Jingtuo Liu and Errui Ding},
  year          = {2021},
  eprint        = {2108.02923},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2108.02923}
}

%%LVLM
@misc{liu2024textmonkeyocrfreelargemultimodal,
  title         = {TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document},
  author        = {Yuliang Liu and Biao Yang and Qiang Liu and Zhang Li and Zhiyin Ma and Shuo Zhang and Xiang Bai},
  year          = {2024},
  eprint        = {2403.04473},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2403.04473}
}

@misc{bai2022wukongreadermultimodalpretrainingfinegrained,
  title         = {Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding},
  author        = {Haoli Bai and Zhiguang Liu and Xiaojun Meng and Wentao Li and Shuang Liu and Nian Xie and Rongfu Zheng and Liangwei Wang and Lu Hou and Jiansheng Wei and Xin Jiang and Qun Liu},
  year          = {2022},
  eprint        = {2212.09621},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2212.09621}
}

@misc{tang2023unifyingvisiontextlayout,
  title         = {Unifying Vision, Text, and Layout for Universal Document Processing},
  author        = {Zineng Tang and Ziyi Yang and Guoxin Wang and Yuwei Fang and Yang Liu and Chenguang Zhu and Michael Zeng and Cha Zhang and Mohit Bansal},
  year          = {2023},
  eprint        = {2212.02623},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2212.02623}
}

@misc{li2023blip2bootstrappinglanguageimagepretraining,
  title         = {BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author        = {Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
  year          = {2023},
  eprint        = {2301.12597},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2301.12597}
}

@misc{ye2024mplugowlmodularizationempowerslarge,
  title         = {mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality},
  author        = {Qinghao Ye and Haiyang Xu and Guohai Xu and Jiabo Ye and Ming Yan and Yiyang Zhou and Junyang Wang and Anwen Hu and Pengcheng Shi and Yaya Shi and Chenliang Li and Yuanhong Xu and Hehong Chen and Junfeng Tian and Qi Qian and Ji Zhang and Fei Huang and Jingren Zhou},
  year          = {2024},
  eprint        = {2304.14178},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2304.14178}
}

@misc{zhang2024llavarenhancedvisualinstruction,
  title         = {LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding},
  author        = {Yanzhe Zhang and Ruiyi Zhang and Jiuxiang Gu and Yufan Zhou and Nedim Lipka and Diyi Yang and Tong Sun},
  year          = {2024},
  eprint        = {2306.17107},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2306.17107}
}

@misc{feng2024docpediaunleashingpowerlarge,
  title         = {DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding},
  author        = {Hao Feng and Qi Liu and Hao Liu and Jingqun Tang and Wengang Zhou and Houqiang Li and Can Huang},
  year          = {2024},
  eprint        = {2311.11810},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2311.11810}
}

@misc{feng2023unidocuniversallargemultimodal,
  title         = {UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding},
  author        = {Hao Feng and Zijian Wang and Jingqun Tang and Jinghui Lu and Wengang Zhou and Houqiang Li and Can Huang},
  year          = {2023},
  eprint        = {2308.11592},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2308.11592}
}


%%MARKER:VLM (general purpose)
@misc{dai2023instructblipgeneralpurposevisionlanguagemodels,
  title         = {InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author        = {Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
  year          = {2023},
  eprint        = {2305.06500},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2305.06500}
}

@misc{li2024monkeyimageresolutiontext,
  title         = {Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models},
  author        = {Zhang Li and Biao Yang and Qiang Liu and Zhiyin Ma and Shuo Zhang and Jingxu Yang and Yabo Sun and Yuliang Liu and Xiang Bai},
  year          = {2024},
  eprint        = {2311.06607},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2311.06607}
}

%MARKER:LAYOUT ANALYSIS
@misc{zhong2019publaynetlargestdatasetdocument,
  title         = {PubLayNet: largest dataset ever for document layout analysis},
  author        = {Xu Zhong and Jianbin Tang and Antonio Jimeno Yepes},
  year          = {2019},
  eprint        = {1908.07836},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1908.07836}
}