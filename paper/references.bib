%SUpermarket related
@misc{ladwig2023,
    title         = {Fine-Grained Product Classification on Leaflet Advertisements},
    author        = {Daniel Ladwig and Bianca Lamm and Janis Keuper},
    year          = {2023},
    eprint        = {2305.03706},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2305.03706}
}

@misc{arroyo2020,
    title         = {Multi-label classification of promotions in digital leaflets using textual and visual information},
    author        = {Roberto Arroyo and David Jiménez-Cabello and Javier Martínez-Cebrián},
    year          = {2020},
    eprint        = {2010.03331},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2010.03331}
}

@article{Zhang2024,
    title     = {PKU-GoodsAD: A Supermarket Goods Dataset for Unsupervised Anomaly Detection and Segmentation},
    volume    = {9},
    issn      = {2377-3774},
    url       = {http://dx.doi.org/10.1109/LRA.2024.3352358},
    doi       = {10.1109/lra.2024.3352358},
    number    = {3},
    journal   = {IEEE Robotics and Automation Letters},
    publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
    author    = {Zhang, Jian and Ding, Runwei and Ban, Miaoju and Dai, Linhui},
    year      = {2024},
    month     = mar,
    pages     = {2008–2015}
}

@misc{nandkumar2024,
    title         = {Enhancing Supermarket Robot Interaction: A Multi-Level LLM Conversational Interface for Handling Diverse Customer Intents},
    author        = {Chandran Nandkumar and Luka Peternel},
    year          = {2024},
    eprint        = {2406.11047},
    archiveprefix = {arXiv},
    primaryclass  = {cs.RO},
    url           = {https://arxiv.org/abs/2406.11047}
}

% MAIN
@misc{kim2022,
    title         = {OCR-free Document Understanding Transformer},
    author        = {Geewook Kim and Teakgyu Hong and Moonbin Yim and Jeongyeon Nam and Jinyoung Park and Jinyeong Yim and Wonseok Hwang and Sangdoo Yun and Dongyoon Han and Seunghyun Park},
    year          = {2022},
    eprint        = {2111.15664},
    archiveprefix = {arXiv},
    primaryclass  = {cs.LG},
    url           = {https://arxiv.org/abs/2111.15664}
}


@misc{qwen2025,
    title         = {Qwen2.5 Technical Report},
    author        = {Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
    year          = {2025},
    eprint        = {2412.15115},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CL},
    url           = {https://arxiv.org/abs/2412.15115}
}

@misc{yao2024,
    title         = {MiniCPM-V: A GPT-4V Level MLLM on Your Phone},
    author        = {Yuan Yao and Tianyu Yu and Ao Zhang and Chongyi Wang and Junbo Cui and Hongji Zhu and Tianchi Cai and Haoyu Li and Weilin Zhao and Zhihui He and Qianyu Chen and Huarong Zhou and Zhensheng Zou and Haoye Zhang and Shengding Hu and Zhi Zheng and Jie Zhou and Jie Cai and Xu Han and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun},
    year          = {2024},
    eprint        = {2408.01800},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2408.01800}
}
%MARKER:OCR
%E2E Pipelines
@inproceedings{tesseract,
    author    = {Smith, R.},
    booktitle = {Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)},
    title     = {An Overview of the Tesseract OCR Engine},
    year      = {2007},
    volume    = {2},
    number    = {},
    pages     = {629-633},
    keywords  = {Optical character recognition software;Search engines;Testing;Open source software;Text recognition;Filters;Prototypes;Independent component analysis;Pipelines;Inspection},
    doi       = {10.1109/ICDAR.2007.4376991}
}

@misc{ppocrv2,
    title         = {PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System},
    author        = {Yuning Du and Chenxia Li and Ruoyu Guo and Cheng Cui and Weiwei Liu and Jun Zhou and Bin Lu and Yehua Yang and Qiwen Liu and Xiaoguang Hu and Dianhai Yu and Yanjun Ma},
    year          = {2021},
    eprint        = {2109.03144},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2109.03144}
}

@misc{doctr,
    title   = {DocTR: Document OCR and Text Recognition},
    author  = {Mindee},
    url     = {https://www.mindee.com/platform/doctr},
    journal = {Mindee}
} 

@misc{easyocr,
    title   = {EasyOCR: Ready-to-use OCR with 80+ languages supported including Chinese, Japanese, Korean and Thai},
    author  = {JaidedAI},
    url     = {https://github.com/JaidedAI/EasyOCR},
    journal = {GitHub}
}


%Text Detection
%DB
@article{liao2019,
    title        = {Real-time Scene Text Detection with Differentiable Binarization},
    url          = {http://arxiv.org/abs/1911.08947},
    doi          = {10.48550/arXiv.1911.08947},
    abstractnote = {Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of DB on five benchmark datasets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by DB are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of ResNet-18, our detector achieves an F-measure of 82.8, running at 62 FPS, on the MSRA-TD500 dataset. Code is available at: https://github.com/MhLiao/DB},
    note         = {arXiv:1911.08947 [cs]},
    number       = {arXiv:1911.08947},
    publisher    = {arXiv},
    author       = {Liao, Minghui and Wan, Zhaoyi and Yao, Cong and Chen, Kai and Bai, Xiang},
    year         = {2019},
    month        = dec
}

%DB++
@misc{liao2022,
    title         = {Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion},
    author        = {Minghui Liao and Zhisheng Zou and Zhaoyi Wan and Cong Yao and Xiang Bai},
    year          = {2022},
    eprint        = {2202.10304},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2202.10304}
}

@misc{characterregionawarenesstext,
    title         = {Character Region Awareness for Text Detection},
    author        = {Youngmin Baek and Bado Lee and Dongyoon Han and Sangdoo Yun and Hwalsuk Lee},
    year          = {2019},
    eprint        = {1904.01941},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/1904.01941}
}

%Text Recognition
@misc{li2022,
    title         = {TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models},
    author        = {Minghao Li and Tengchao Lv and Jingye Chen and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},
    year          = {2022},
    eprint        = {2109.10282},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CL},
    url           = {https://arxiv.org/abs/2109.10282}
}

%%Scene TR
@misc{shi2015,
    title         = {An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition},
    author        = {Baoguang Shi and Xiang Bai and Cong Yao},
    year          = {2015},
    eprint        = {1507.05717},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/1507.05717}
}

@misc{gao2017,
    title         = {Reading Scene Text with Attention Convolutional Sequence Modeling},
    author        = {Yunze Gao and Yingying Chen and Jinqiao Wang and Hanqing Lu},
    year          = {2017},
    eprint        = {1709.04303},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/1709.04303}
}

@misc{sheng2019,
    title         = {NRTR: A No-Recurrence Sequence-to-Sequence Model For Scene Text Recognition},
    author        = {Fenfen Sheng and Zhineng Chen and Bo Xu},
    year          = {2019},
    eprint        = {1806.00926},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/1806.00926}
}

%%Rectification
@article{shi2019,
    author   = {Shi, Baoguang and Yang, Mingkun and Wang, Xinggang and Lyu, Pengyuan and Yao, Cong and Bai, Xiang},
    journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    title    = {ASTER: An Attentional Scene Text Recognizer with Flexible Rectification},
    year     = {2019},
    volume   = {41},
    number   = {9},
    pages    = {2035-2048},
    keywords = {Text recognition;Character recognition;Detectors;Decoding;Recurrent neural networks;Proposals;Scene text recognition;thin-plate spline;image transformation;sequence-to-sequence learning},
    doi      = {10.1109/TPAMI.2018.2848939}
}

@misc{zhan2019,
    title         = {ESIR: End-to-end Scene Text Recognition via Iterative Image Rectification},
    author        = {Fangneng Zhan and Shijian Lu},
    year          = {2019},
    eprint        = {1812.05824},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/1812.05824}
}

%Deskew
@inproceedings{pham2022,
    author    = {Pham, Luan and Hoang, Phu Hao and Mai, Xuan Toan and Tran, Tuan Anh},
    booktitle = {2022 IEEE International Conference on Image Processing (ICIP)},
    title     = {Adaptive Radial Projection on Fourier Magnitude Spectrum for Document Image Skew Estimation},
    year      = {2022},
    volume    = {},
    number    = {},
    pages     = {1061-1065},
    keywords  = {Codes;Image processing;Estimation;Reliability;Task analysis;Document Image Skew Estimation;Fourier Transform-based method;Adaptive Radial Projection},
    doi       = {10.1109/ICIP46576.2022.9897910}
}

%Denoising
@inproceedings{zhao2018,
    author    = {Zhao, Guoping and Liu, Jiajun and Jiang, Jiacheng and Guan, Hua and Wen, Ji-Rong},
    booktitle = {2018 24th International Conference on Pattern Recognition (ICPR)},
    title     = {Skip-Connected Deep Convolutional Autoencoder for Restoration of Document Images},
    year      = {2018},
    volume    = {},
    number    = {},
    pages     = {2935-2940},
    keywords  = {Image restoration;Convolution;Decoding;Noise reduction;Kernel;Optical character recognition software;Task analysis},
    doi       = {10.1109/ICPR.2018.8546199}
}

@misc{gangeh2021,
    title         = {End-to-End Unsupervised Document Image Blind Denoising},
    author        = {Mehrdad J Gangeh and Marcin Plata and Hamid Motahari and Nigel P Duffy},
    year          = {2021},
    eprint        = {2105.09437},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2105.09437}
}

%MARKER: LLM
@misc{brown2020,
    title         = {Language Models are Few-Shot Learners},
    author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year          = {2020},
    eprint        = {2005.14165},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CL},
    url           = {https://arxiv.org/abs/2005.14165}
}

@misc{touvron2023,
    title         = {LLaMA: Open and Efficient Foundation Language Models},
    author        = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
    year          = {2023},
    eprint        = {2302.13971},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CL},
    url           = {https://arxiv.org/abs/2302.13971}
}

@misc{yenduri2023,
    title         = {Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions},
    author        = {Gokul Yenduri and Ramalingam M and Chemmalar Selvi G and Supriya Y and Gautam Srivastava and Praveen Kumar Reddy Maddikunta and Deepti Raj G and Rutvij H Jhaveri and Prabadevi B and Weizheng Wang and Athanasios V. Vasilakos and Thippa Reddy Gadekallu},
    year          = {2023},
    eprint        = {2305.10435},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CL},
    url           = {https://arxiv.org/abs/2305.10435}
}

%MARKER:VISUAL DOCUMENT UNDERSTANDING (VQA, OCR)
%% E2E
@misc{vaswani2017,
    title         = {Attention Is All You Need},
    author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year          = {2023},
    eprint        = {1706.03762},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CL},
    url           = {https://arxiv.org/abs/1706.03762}
}

@misc{dosovitskiy2021,
    title         = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author        = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    year          = {2021},
    eprint        = {2010.11929},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2010.11929}
}


@misc{appalaraju2021,
    title         = {DocFormer: End-to-End Transformer for Document Understanding},
    author        = {Srikar Appalaraju and Bhavan Jasani and Bhargava Urala Kota and Yusheng Xie and R. Manmatha},
    year          = {2021},
    eprint        = {2106.11539},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2106.11539}
}

@misc{peng2022,
    title         = {ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding},
    author        = {Qiming Peng and Yinxu Pan and Wenjin Wang and Bin Luo and Zhenyu Zhang and Zhengjie Huang and Teng Hu and Weichong Yin and Yongfeng Chen and Yin Zhang and Shikun Feng and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},
    year          = {2022},
    eprint        = {2210.06155},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CL},
    url           = {https://arxiv.org/abs/2210.06155}
}

@misc{li2021,
    title         = {StrucTexT: Structured Text Understanding with Multi-Modal Transformers},
    author        = {Yulin Li and Yuxi Qian and Yuchen Yu and Xiameng Qin and Chengquan Zhang and Yan Liu and Kun Yao and Junyu Han and Jingtuo Liu and Errui Ding},
    year          = {2021},
    eprint        = {2108.02923},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2108.02923}
}

%%LVLM
@misc{liu2024textmonkeyocrfreelargemultimodal,
    title         = {TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document},
    author        = {Yuliang Liu and Biao Yang and Qiang Liu and Zhang Li and Zhiyin Ma and Shuo Zhang and Xiang Bai},
    year          = {2024},
    eprint        = {2403.04473},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2403.04473}
}

@misc{bai2022wukongreadermultimodalpretrainingfinegrained,
    title         = {Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding},
    author        = {Haoli Bai and Zhiguang Liu and Xiaojun Meng and Wentao Li and Shuang Liu and Nian Xie and Rongfu Zheng and Liangwei Wang and Lu Hou and Jiansheng Wei and Xin Jiang and Qun Liu},
    year          = {2022},
    eprint        = {2212.09621},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CL},
    url           = {https://arxiv.org/abs/2212.09621}
}

@misc{tang2023unifyingvisiontextlayout,
    title         = {Unifying Vision, Text, and Layout for Universal Document Processing},
    author        = {Zineng Tang and Ziyi Yang and Guoxin Wang and Yuwei Fang and Yang Liu and Chenguang Zhu and Michael Zeng and Cha Zhang and Mohit Bansal},
    year          = {2023},
    eprint        = {2212.02623},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2212.02623}
}

@misc{li2023blip2bootstrappinglanguageimagepretraining,
    title         = {BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
    author        = {Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
    year          = {2023},
    eprint        = {2301.12597},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2301.12597}
}

@misc{ye2024mplugowlmodularizationempowerslarge,
    title         = {mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality},
    author        = {Qinghao Ye and Haiyang Xu and Guohai Xu and Jiabo Ye and Ming Yan and Yiyang Zhou and Junyang Wang and Anwen Hu and Pengcheng Shi and Yaya Shi and Chenliang Li and Yuanhong Xu and Hehong Chen and Junfeng Tian and Qi Qian and Ji Zhang and Fei Huang and Jingren Zhou},
    year          = {2024},
    eprint        = {2304.14178},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CL},
    url           = {https://arxiv.org/abs/2304.14178}
}

@misc{zhang2024llavarenhancedvisualinstruction,
    title         = {LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding},
    author        = {Yanzhe Zhang and Ruiyi Zhang and Jiuxiang Gu and Yufan Zhou and Nedim Lipka and Diyi Yang and Tong Sun},
    year          = {2024},
    eprint        = {2306.17107},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2306.17107}
}

@misc{feng2024docpediaunleashingpowerlarge,
    title         = {DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding},
    author        = {Hao Feng and Qi Liu and Hao Liu and Jingqun Tang and Wengang Zhou and Houqiang Li and Can Huang},
    year          = {2024},
    eprint        = {2311.11810},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2311.11810}
}

@misc{feng2023unidocuniversallargemultimodal,
    title         = {UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding},
    author        = {Hao Feng and Zijian Wang and Jingqun Tang and Jinghui Lu and Wengang Zhou and Houqiang Li and Can Huang},
    year          = {2023},
    eprint        = {2308.11592},
    archiveprefix = {arXiv},
    primaryclass  = {cs.AI},
    url           = {https://arxiv.org/abs/2308.11592}
}


%%MARKER:VLM (general purpose)
@misc{dai2023,
    title         = {InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
    author        = {Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
    year          = {2023},
    eprint        = {2305.06500},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2305.06500}
}

@misc{li2024,
    title         = {Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models},
    author        = {Zhang Li and Biao Yang and Qiang Liu and Zhiyin Ma and Shuo Zhang and Jingxu Yang and Yabo Sun and Yuliang Liu and Xiang Bai},
    year          = {2024},
    eprint        = {2311.06607},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2311.06607}
}


@misc{wei2024,
    title         = {General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model},
    author        = {Haoran Wei and Chenglong Liu and Jinyue Chen and Jia Wang and Lingyu Kong and Yanming Xu and Zheng Ge and Liang Zhao and Jianjian Sun and Yuang Peng and Chunrui Han and Xiangyu Zhang},
    year          = {2024},
    eprint        = {2409.01704},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CV},
    url           = {https://arxiv.org/abs/2409.01704}
}
%MARKER:LAYOUT ANALYSIS
@misc{zhong2019publaynetlargestdatasetdocument,
    title         = {PubLayNet: largest dataset ever for document layout analysis},
    author        = {Xu Zhong and Jianbin Tang and Antonio Jimeno Yepes},
    year          = {2019},
    eprint        = {1908.07836},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CL},
    url           = {https://arxiv.org/abs/1908.07836}
}



% Object Detection and Segmentation
@misc{zhuDeformableDETRDeformable2021,
  title = {Deformable {{DETR}}: {{Deformable Transformers}} for {{End-to-End Object Detection}}},
  shorttitle = {Deformable {{DETR}}},
  author = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  year = {2021},
  eprint = {2010.04159},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.04159},
  url = {http://arxiv.org/abs/2010.04159},
  urldate = {2024-12-13},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion}
}

@misc{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  eprint = {1506.02640},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02640},
  url = {http://arxiv.org/abs/1506.02640},
  urldate = {2024-12-12},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion}
}

% Faster R-CNN
@misc{renFasterRCNNRealTime2016,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2016},
  eprint = {1506.01497},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.01497},
  url = {http://arxiv.org/abs/1506.01497},
  urldate = {2024-12-13},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion}
}

% Mask R-CNN
@misc{heMaskRCNN2018,
  title = {Mask {{R-CNN}}},
  author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2018},
  eprint = {1703.06870},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.06870},
  url = {http://arxiv.org/abs/1703.06870},
  urldate = {2024-12-13},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion}
}

@misc{mohamedINSTAYOLORealTimeInstance2024,
  title = {{{INSTA-YOLO}}: {{Real-Time Instance Segmentation}}},
  shorttitle = {{{INSTA-YOLO}}},
  author = {Mohamed, Eslam and Shaker, Abdelrahman and {El-Sallab}, Ahmad and Hadhoud, Mayada},
  year = {2024},
  eprint = {2102.06777},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.06777},
  url = {http://arxiv.org/abs/2102.06777},
  urldate = {2025-02-26},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

% RT-DETR
@misc{zhaoDETRsBeatYOLOs2024a,
  title = {{{DETRs Beat YOLOs}} on {{Real-time Object Detection}}},
  author = {Zhao, Yian and Lv, Wenyu and Xu, Shangliang and Wei, Jinman and Wang, Guanzhong and Dang, Qingqing and Liu, Yi and Chen, Jie},
  year = {2024},
  eprint = {2304.08069},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.08069},
  url = {http://arxiv.org/abs/2304.08069},
  urldate = {2025-02-26},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

% R-CNN
@misc{girshickRichFeatureHierarchies2014a,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  eprint = {1311.2524},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1311.2524},
  url = {http://arxiv.org/abs/1311.2524},
  urldate = {2025-02-26},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

% Fast RCNN
@misc{girshickFastRCNN2015,
  title = {Fast {{R-CNN}}},
  author = {Girshick, Ross},
  year = {2015},
  eprint = {1504.08083},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1504.08083},
  url = {http://arxiv.org/abs/1504.08083},
  urldate = {2024-12-13},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion}
}

%  Selective Search
@article{uijlingsSelectiveSearchObject2013,
  title = {Selective {{Search}} for {{Object Recognition}}},
  author = {Uijlings, J. R. R. and Van De Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
  year = {2013},
  journal = {International Journal of Computer Vision},
  volume = {104},
  pages = {154--171},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-013-0620-5},
  url = {http://link.springer.com/10.1007/s11263-013-0620-5},
  urldate = {2024-12-13},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  keywords = {notion}
}




